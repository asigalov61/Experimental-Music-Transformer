{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "gradient": {
          "editing": false,
          "id": "ac5a4cf0-d9d2-47b5-9633-b53f8d99a4d2",
          "kernelId": ""
        },
        "id": "SiTIpPjArIyr"
      },
      "source": [
        "# Experimental Music Transformer Version 3 (ver. 0.5)\n",
        "\n",
        "***\n",
        "\n",
        "Powered by tegridy-tools: https://github.com/asigalov61/tegridy-tools\n",
        "\n",
        "***\n",
        "\n",
        "WARNING: This complete implementation is a functioning model of the Artificial Intelligence. Please excercise great humility, care, and respect. https://www.nscai.gov/\n",
        "\n",
        "***\n",
        "\n",
        "#### Project Los Angeles\n",
        "\n",
        "#### Tegridy Code 2023\n",
        "\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "gradient": {
          "editing": false,
          "id": "fa0a611c-1803-42ae-bdf6-a49b5a4e781b",
          "kernelId": ""
        },
        "id": "gOd93yV0sGd2"
      },
      "source": [
        "# (SETUP ENVIRONMENT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "gradient": {
          "editing": false,
          "id": "a1a45a91-d909-4fd4-b67a-5e16b971d179",
          "kernelId": ""
        },
        "id": "fX12Yquyuihc"
      },
      "outputs": [],
      "source": [
        "#@title Install all dependencies (run only once per session)\n",
        "\n",
        "!git clone https://github.com/asigalov61/tegridy-tools\n",
        "!pip install einops\n",
        "!pip install torch-summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "gradient": {
          "editing": false,
          "id": "b8207b76-9514-4c07-95db-95a4742e52c5",
          "kernelId": ""
        },
        "id": "z7n9vnKmug1J"
      },
      "outputs": [],
      "source": [
        "#@title Import all needed modules\n",
        "\n",
        "print('Loading needed modules. Please wait...')\n",
        "\n",
        "import os\n",
        "import pickle\n",
        "import copy\n",
        "import statistics\n",
        "import secrets\n",
        "import tqdm\n",
        "import math\n",
        "\n",
        "from joblib import Parallel, delayed, parallel_config\n",
        "\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "from torch.optim import Adam\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "torch.set_float32_matmul_precision('high')\n",
        "torch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul\n",
        "torch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from torchsummary import summary\n",
        "from sklearn import metrics\n",
        "\n",
        "print('Loading TMIDIX module...')\n",
        "\n",
        "%cd /content/tegridy-tools/tegridy-tools/\n",
        "\n",
        "import TMIDIX\n",
        "\n",
        "print('Loading X Transformer module...')\n",
        "\n",
        "%cd /content/tegridy-tools/tegridy-tools/X-Transformer\n",
        "\n",
        "from x_transformer_1_23_2 import *\n",
        "import random\n",
        "\n",
        "%cd /content/\n",
        "\n",
        "print('Creating I/O dirs...')\n",
        "\n",
        "if not os.path.exists('/content/Dataset'):\n",
        "    os.makedirs('/content/Dataset')\n",
        "\n",
        "if not os.path.exists('/content/DATA'):\n",
        "    os.makedirs('/content/DATA')\n",
        "\n",
        "print('Done!')\n",
        "print('PyTorch version:', torch.__version__)\n",
        "print('Enjoy! :)')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IDybEm0PgkW4"
      },
      "source": [
        "# (DOWNLOAD AND UNZIP MIDI DATASET)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Download and unzip Mono Melodies Piano Violin MIDI Dataset\n",
        "%cd /content/Dataset\n",
        "!wget https://github.com/asigalov61/Tegridy-MIDI-Dataset/raw/master/Mono-Melodies/Piano-Violin/Mono-Melodies-Piano-Violin-CC-BY-NC-SA.zip.001\n",
        "!wget https://github.com/asigalov61/Tegridy-MIDI-Dataset/raw/master/Mono-Melodies/Piano-Violin/Mono-Melodies-Piano-Violin-CC-BY-NC-SA.zip.002\n",
        "!cat Mono-Melodies-Piano-Violin-CC-BY-NC-SA.zip* > Mono-Melodies-Piano-Violin-CC-BY-NC-SA.zip\n",
        "!unzip Mono-Melodies-Piano-Violin-CC-BY-NC-SA.zip\n",
        "!rm Mono-Melodies-Piano-Violin-CC-BY-NC-SA.zip\n",
        "%cd /content/"
      ],
      "metadata": {
        "cellView": "form",
        "id": "qH-JN_V-epO4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# (LOAD MIDI PROCESSOR)"
      ],
      "metadata": {
        "id": "sjfKnkof1Oto"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HkS8pYJBPSFN",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title TMIDIX MIDI Processor\n",
        "\n",
        "print('=' * 70)\n",
        "print('Loading TMIDIX MIDI Processor...')\n",
        "print('=' * 70)\n",
        "\n",
        "def TMIDIX_MIDI_Processor(midi_file):\n",
        "\n",
        "    melody_chords = []\n",
        "\n",
        "    try:\n",
        "\n",
        "        fn = os.path.basename(midi_file)\n",
        "\n",
        "        # Filtering out GIANT4 MIDIs\n",
        "        file_size = os.path.getsize(midi_file)\n",
        "\n",
        "        if file_size <= 1000000:\n",
        "\n",
        "          #=======================================================\n",
        "          # START PROCESSING\n",
        "\n",
        "          # Convering MIDI to ms score with MIDI.py module\n",
        "          score = TMIDIX.midi2single_track_ms_score(open(midi_file, 'rb').read(), recalculate_channels=False)\n",
        "\n",
        "          # INSTRUMENTS CONVERSION CYCLE\n",
        "          events_matrix = []\n",
        "          itrack = 1\n",
        "          patches = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
        "\n",
        "          while itrack < len(score):\n",
        "              for event in score[itrack]:\n",
        "                  if event[0] == 'note' or event[0] == 'patch_change':\n",
        "                      events_matrix.append(event)\n",
        "              itrack += 1\n",
        "\n",
        "          events_matrix.sort(key=lambda x: x[1])\n",
        "\n",
        "          events_matrix1 = []\n",
        "\n",
        "          for event in events_matrix:\n",
        "                  if event[0] == 'patch_change':\n",
        "                        patches[event[2]] = event[3]\n",
        "\n",
        "                  if event[0] == 'note':\n",
        "                        event.extend([patches[event[3]]])\n",
        "\n",
        "                        if events_matrix1:\n",
        "                            if (event[1] == events_matrix1[-1][1]):\n",
        "                                if ([event[3], event[4]] != events_matrix1[-1][3:5]):\n",
        "                                    events_matrix1.append(event)\n",
        "                            else:\n",
        "                                events_matrix1.append(event)\n",
        "\n",
        "                        else:\n",
        "                            events_matrix1.append(event)\n",
        "\n",
        "        if len(events_matrix1) > 0:\n",
        "            if min([e[1] for e in events_matrix1]) >= 0 and min([e[2] for e in events_matrix1]) >= 0:\n",
        "\n",
        "                #=======================================================\n",
        "                # PRE-PROCESSING\n",
        "\n",
        "                # checking number of instruments in a composition\n",
        "                instruments_list = list(set([y[3] for y in events_matrix1]))\n",
        "\n",
        "                if len(events_matrix1) > 0:\n",
        "\n",
        "                    #===================================\n",
        "                    # ORIGINAL COMPOSITION\n",
        "                    #===================================\n",
        "\n",
        "                    # Adjusting timings\n",
        "\n",
        "                    for e in events_matrix1:\n",
        "                      e[1] = int(e[1] / 16)\n",
        "                      e[2] = int(e[2] / 16)\n",
        "\n",
        "                    # Sorting by patch, pitch, then by start-time\n",
        "\n",
        "                    events_matrix1.sort(key=lambda x: x[6])\n",
        "                    events_matrix1.sort(key=lambda x: x[4], reverse=True)\n",
        "                    events_matrix1.sort(key=lambda x: x[1])\n",
        "\n",
        "                    #=======================================================\n",
        "                    # FINAL PROCESSING\n",
        "\n",
        "                    #=======================================================\n",
        "                    # MAIN PROCESSING CYCLE\n",
        "                    #=======================================================\n",
        "\n",
        "                    pe = events_matrix1[0]\n",
        "\n",
        "                    notes = []\n",
        "\n",
        "                    for e in events_matrix1:\n",
        "\n",
        "                      time = max(0, min(255, (e[1] - pe[1])))\n",
        "                      dur = max(0, min(255, e[2]))\n",
        "                      cha = max(0, min(15, e[3]))\n",
        "                      ptc = max(1, min(127, e[4]))\n",
        "\n",
        "                      notes.append([time, dur, cha, ptc])\n",
        "\n",
        "                      pe = e\n",
        "\n",
        "                    chords = []\n",
        "                    cho = []\n",
        "\n",
        "                    for n in notes:\n",
        "\n",
        "                      if n[2] not in [0, 3]:\n",
        "                        n[2] = 0\n",
        "\n",
        "                      if n[0] == 0:\n",
        "                        chans = list(set([nn[2] for nn in cho]))\n",
        "                        if (n[2] == 3) and (3 in chans):\n",
        "                          n[2] = 0\n",
        "\n",
        "                        cho.append(n)\n",
        "                      else:\n",
        "                        if len(cho) > 0:\n",
        "                          chords.append(cho)\n",
        "\n",
        "                        cho = []\n",
        "                        cho.append(n)\n",
        "\n",
        "\n",
        "                    if len(cho) > 0:\n",
        "                      chords.append(cho)\n",
        "\n",
        "                    return chords\n",
        "\n",
        "    except:\n",
        "      return None\n",
        "\n",
        "print('Done!')\n",
        "print('=' * 70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JwrqQeie08t0"
      },
      "source": [
        "# (FILES LIST)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "1s_9bVGrPU_q"
      },
      "outputs": [],
      "source": [
        "#@title Save file list\n",
        "###########\n",
        "\n",
        "print('=' * 70)\n",
        "print('Loading MIDI files...')\n",
        "print('This may take a while on a large dataset in particular.')\n",
        "\n",
        "dataset_addr = \"/content/Dataset\"\n",
        "\n",
        "# os.chdir(dataset_addr)\n",
        "filez = list()\n",
        "for (dirpath, dirnames, filenames) in os.walk(dataset_addr):\n",
        "    filez += [os.path.join(dirpath, file) for file in filenames]\n",
        "print('=' * 70)\n",
        "\n",
        "if not filez:\n",
        "    print('Could not find any MIDI files. Please check Dataset dir...')\n",
        "    print('=' * 70)\n",
        "\n",
        "else:\n",
        "  print('Randomizing file list...')\n",
        "  random.shuffle(filez)\n",
        "  print('Done!')\n",
        "  print('=' * 70)\n",
        "  print('Total files:', len(filez))\n",
        "  print('=' * 70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FLxHvO-wlwfU"
      },
      "source": [
        "# (PROCESS MIDIs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nFkq_d4nPVmW",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Process MIDIs with TMIDIX MIDI processor\n",
        "\n",
        "print('=' * 70)\n",
        "print('TMIDIX MIDI Processor')\n",
        "print('=' * 70)\n",
        "print('Starting up...')\n",
        "print('=' * 70)\n",
        "\n",
        "###########\n",
        "\n",
        "melody_chords_f = []\n",
        "\n",
        "print('Processing MIDI files. Please wait...')\n",
        "print('=' * 70)\n",
        "\n",
        "for i in tqdm.tqdm(range(0, len(filez), 16)):\n",
        "\n",
        "  with parallel_config(backend='threading', n_jobs=4, verbose = 0):\n",
        "\n",
        "    output = Parallel()(delayed(TMIDIX_MIDI_Processor)(f) for f in filez[i:i+16])\n",
        "\n",
        "    for o in output:\n",
        "        if o is not None:\n",
        "            melody_chords_f.append(o)\n",
        "\n",
        "print('Done!')\n",
        "print('=' * 70)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# (SAVE/LOAD PROCESSED MIDIs)"
      ],
      "metadata": {
        "id": "JIVuL6Wr16nW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zjGwEKInQT8a",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Save processed MIDIs\n",
        "TMIDIX.Tegridy_Any_Pickle_File_Writer(melody_chords_f, '/content/DATA/Processed_MIDIs')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lZGSjKe9QUey",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Load processed MIDIs\n",
        "melody_chords_f = TMIDIX.Tegridy_Any_Pickle_File_Reader('/content/DATA/Processed_MIDIs')\n",
        "print('Done!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4hlZohOTSKqq"
      },
      "source": [
        "# (PREP INTs)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Convert processed MIDIs to INTs for training\n",
        "\n",
        "def split_data_into_chunks(data, chunk_size):\n",
        "    # Use list comprehension to create chunks of the specified size\n",
        "    return [data[i:i + chunk_size] for i in range(0, len(data) - len(data) % chunk_size, chunk_size)]\n",
        "\n",
        "print('=' * 70)\n",
        "\n",
        "SEQ_LEN = 256\n",
        "\n",
        "src_list = []\n",
        "trg_list = []\n",
        "\n",
        "for m in tqdm.tqdm(melody_chords_f):\n",
        "\n",
        "  for j in range(-6, 6):\n",
        "\n",
        "    sdat = []\n",
        "    tdat = []\n",
        "\n",
        "    for mmm in m:\n",
        "\n",
        "      melody_tone = max(1, min(127, mmm[0][3]+j)) % 12\n",
        "\n",
        "      tones_chord = sorted(list(set([(n[3]+j) % 12 for n in mmm])))\n",
        "\n",
        "      try:\n",
        "        chord_id = TMIDIX.ALL_CHORDS.index(tones_chord)\n",
        "      except:\n",
        "        chord_id = -1\n",
        "\n",
        "      if chord_id != -1:\n",
        "        pchord_id = chord_id\n",
        "\n",
        "      else:\n",
        "        pchord_id = TMIDIX.ALL_CHORDS.index([melody_tone])\n",
        "\n",
        "      sdat.extend([melody_tone])\n",
        "      tdat.extend([pchord_id])\n",
        "\n",
        "    if len(sdat) > SEQ_LEN:\n",
        "      sdat1 = split_data_into_chunks(sdat, SEQ_LEN)\n",
        "\n",
        "    if len(tdat) > SEQ_LEN:\n",
        "      tdat1 = split_data_into_chunks(tdat, SEQ_LEN)\n",
        "\n",
        "    for i in range(len(tdat1)):\n",
        "      fill_ratio = sum(1 for t in tdat1[i] if t > 11) / len(tdat1[i])\n",
        "\n",
        "      if fill_ratio >= 0.6:\n",
        "        src_list.append(sdat1[i])\n",
        "        trg_list.append(tdat1[i])\n",
        "\n",
        "print('Done!')\n",
        "print('=' * 70)\n",
        "if len(max(src_list, key=len)) == len(min(src_list, key=len)) and len(max(trg_list, key=len)) == len(min(trg_list, key=len)):\n",
        "  print('All data is good!')\n",
        "else:\n",
        "  print('WARNING!!! BAD DATA!!!')\n",
        "print('=' * 70)"
      ],
      "metadata": {
        "id": "5_UFw-JDlh0a",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trg_list[0]"
      ],
      "metadata": {
        "id": "m8HRAdUK9UaI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#==========================================================="
      ],
      "metadata": {
        "id": "RV26c42loxvr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# (SAVE/LOAD TRAINING INTs)"
      ],
      "metadata": {
        "id": "3FLw4Pyy20nK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z-omEbFmVlOI",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Save INTs\n",
        "TMIDIX.Tegridy_Any_Pickle_File_Writer([src_list, trg_list], '/content/DATA/Training_INTs')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R7FtGA4lZ8ub",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Load INTs\n",
        "src_list, trg_list = TMIDIX.Tegridy_Any_Pickle_File_Reader('/content/DATA/Training_INTs')\n",
        "print('Done!')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# (PREP DATA)"
      ],
      "metadata": {
        "id": "GffpdFhb34dc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title DATA\n",
        "SEQ_LEN = 256\n",
        "batch_size = 128\n",
        "\n",
        "def train_test_split(src, trg, test_size=0.2):\n",
        "    indices = torch.randperm(len(src)).tolist()\n",
        "    split = int(test_size * len(src))\n",
        "    src_train = src[indices[split:]]\n",
        "    trg_train = trg[indices[split:]]\n",
        "    src_test = src[indices[:split]]\n",
        "    trg_test = trg[indices[:split]]\n",
        "    return src_train, src_test, trg_train, trg_test\n",
        "\n",
        "# Convert lists to PyTorch tensors\n",
        "src_in = torch.tensor(src_list).long()\n",
        "trg_in = torch.tensor(trg_list).long()\n",
        "\n",
        "# Split the data into train, validation, and test sets\n",
        "src_train, src_val_test, trg_train, trg_val_test = train_test_split(src_in, trg_in, test_size=0.05)\n",
        "src_val, src_test, trg_val, trg_test = train_test_split(src_val_test, trg_val_test, test_size=0.3)\n",
        "\n",
        "class MusicDataset(Dataset):\n",
        "    def __init__(self, src, trg):\n",
        "        self.src = src\n",
        "        self.trg = trg\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.src)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        src = self.src[idx].long()\n",
        "        trg = self.trg[idx].long()\n",
        "        return src, trg\n",
        "\n",
        "# Create datasets for each split\n",
        "train_dataset = MusicDataset(src_train, trg_train)\n",
        "val_dataset = MusicDataset(src_val, trg_val)\n",
        "test_dataset = MusicDataset(src_test, trg_test)\n",
        "\n",
        "# Create data loaders for each split\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2, drop_last=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2, drop_last=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2, drop_last=True)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "KPoZ434jJF08"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset[0]"
      ],
      "metadata": {
        "id": "4ulzw72WJ1P7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# (TRAIN)"
      ],
      "metadata": {
        "id": "4e15_GQB4IMU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Train model\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Initialize the model\n",
        "model = XTransformer(\n",
        "    dim = 512,\n",
        "    enc_num_tokens = 12,\n",
        "    enc_depth = 8,\n",
        "    enc_heads = 8,\n",
        "    enc_max_seq_len = SEQ_LEN,\n",
        "    enc_dropout = 0.3,\n",
        "    enc_attn_flash = True,\n",
        "    dec_num_tokens = 320,\n",
        "    dec_depth = 8,\n",
        "    dec_heads = 8,\n",
        "    dec_max_seq_len = SEQ_LEN,\n",
        "    dec_dropout = 0.3,\n",
        "    dec_attn_flash = True,\n",
        "    cross_attn_tokens_dropout = 0.3\n",
        "    )\n",
        "\n",
        "model.cuda()\n",
        "\n",
        "# Define the optimizer\n",
        "optimizer = Adam(model.parameters())\n",
        "\n",
        "# Initialize AMP\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "NUM_EPOCHS = 100\n",
        "GRADIENT_ACCUMULATE_EVERY = 1  # Set the number of steps to accumulate gradients\n",
        "\n",
        "PRINT_STATS_EVERY = 20\n",
        "\n",
        "train_losses = []\n",
        "train_accs = []\n",
        "\n",
        "num_steps = 0\n",
        "\n",
        "# Training loop with gradient accumulation\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):  # replace NUM_EPOCHS with the actual number of epochs\n",
        "    model.train()  # set the model to training mode\n",
        "    total_loss = 0\n",
        "    optimizer.zero_grad(set_to_none=True)  # Initialize gradients to zero at the start of the epoch\n",
        "\n",
        "    for batch_idx, batch in enumerate(tqdm.tqdm(train_loader)):  # iterate over batches of data\n",
        "        src, tgt = [item.cuda() for item in batch]  # unpack the source and target tensors from the current batch\n",
        "\n",
        "        src_mask = src.bool().cuda()  # create a mask for the source sequence\n",
        "        with torch.cuda.amp.autocast():\n",
        "            loss, acc = model(src, tgt, mask=src_mask)  # forward pass\n",
        "\n",
        "        # loss = loss / GRADIENT_ACCUMULATE_EVERY  # Normalize the loss by the number of accumulation steps\n",
        "        scaler.scale(loss).backward()  # Backward pass with gradient scaling\n",
        "\n",
        "        train_losses.append(loss.mean().item() * GRADIENT_ACCUMULATE_EVERY)\n",
        "        train_accs.append(acc.mean().item())\n",
        "\n",
        "\n",
        "        if (batch_idx + 1) % GRADIENT_ACCUMULATE_EVERY == 0:  # Perform optimization step after accumulating gradients\n",
        "            scaler.unscale_(optimizer)\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "            optimizer.zero_grad(set_to_none=True)  # Reset gradients after optimization step\n",
        "\n",
        "        total_loss += loss.item() * GRADIENT_ACCUMULATE_EVERY  # Undo the normalization for logging\n",
        "\n",
        "        if num_steps % PRINT_STATS_EVERY == 0:\n",
        "            print(f'Training Loss: {total_loss / (batch_idx + 1)}, Accuracy: {acc.item()}')\n",
        "\n",
        "        num_steps += 1\n",
        "\n",
        "    # Validation loop\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for i, (src, trg) in enumerate(val_loader):\n",
        "            src = src.cuda()\n",
        "            trg = trg.cuda()\n",
        "            src_mask = src.bool().cuda()  # create a mask for the source sequence\n",
        "            with torch.cuda.amp.autocast():\n",
        "              loss, acc = model(src, tgt, mask=src_mask)  # forward pass\n",
        "\n",
        "            print(f'Validation Loss: {loss.item()}, Accuracy: {acc.item()}')\n",
        "\n",
        "            if i > 10:\n",
        "              break\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)  # calculate average loss for the epoch\n",
        "    print(f'Epoch {epoch}: Average Loss: {avg_loss}')\n",
        "\n",
        "\n",
        "    print('Plotting training loss graph...')\n",
        "\n",
        "    tr_loss_list = train_losses\n",
        "    plt.plot([i for i in range(len(tr_loss_list))] ,tr_loss_list, 'b')\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "    print('Done!')\n",
        "\n",
        "    print('Plotting training acc graph...')\n",
        "\n",
        "    tr_loss_list = train_accs\n",
        "    plt.plot([i for i in range(len(tr_loss_list))] ,tr_loss_list, 'b')\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "    print('Done!')"
      ],
      "metadata": {
        "cellView": "form",
        "id": "xysEmuazmDxy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGu6ivTtrb_Z"
      },
      "source": [
        "# EVAL"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Validation loop\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for i, (src, trg) in enumerate(test_loader):\n",
        "        src = src.cuda()\n",
        "        trg = trg.cuda()\n",
        "        src_mask = src.bool().cuda()  # create a mask for the source sequence\n",
        "        with torch.cuda.amp.autocast():\n",
        "          loss, acc = model(src, tgt, mask=src_mask)  # forward pass\n",
        "\n",
        "        print(f'Validation Loss: {loss.item()}, Accuracy: {acc.item()}')"
      ],
      "metadata": {
        "id": "qfLnCq9NKGn5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#==========================================================="
      ],
      "metadata": {
        "id": "vd9J3jQKzF63"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convering MIDI to ms score with MIDI.py module\n",
        "\n",
        "midi_file = '/content/tegridy-tools/tegridy-tools/seed-melody.mid'\n",
        "\n",
        "score = TMIDIX.midi2single_track_ms_score(open(midi_file, 'rb').read(), recalculate_channels=False)\n",
        "\n",
        "# INSTRUMENTS CONVERSION CYCLE\n",
        "events_matrix = []\n",
        "itrack = 1\n",
        "patches = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
        "\n",
        "while itrack < len(score):\n",
        "    for event in score[itrack]:\n",
        "        if event[0] == 'note' or event[0] == 'patch_change':\n",
        "            events_matrix.append(event)\n",
        "    itrack += 1\n",
        "\n",
        "events_matrix.sort(key=lambda x: x[1])\n",
        "\n",
        "events_matrix1 = []\n",
        "\n",
        "for event in events_matrix:\n",
        "        if event[0] == 'patch_change':\n",
        "              patches[event[2]] = event[3]\n",
        "\n",
        "        if event[0] == 'note':\n",
        "              event.extend([patches[event[3]]])\n",
        "\n",
        "              if events_matrix1:\n",
        "                  if (event[1] == events_matrix1[-1][1]):\n",
        "                      if ([event[3], event[4]] != events_matrix1[-1][3:5]):\n",
        "                          events_matrix1.append(event)\n",
        "                  else:\n",
        "                      events_matrix1.append(event)\n",
        "\n",
        "              else:\n",
        "                  events_matrix1.append(event)\n",
        "\n",
        "if len(events_matrix1) > 0:\n",
        "  if min([e[1] for e in events_matrix1]) >= 0 and min([e[2] for e in events_matrix1]) >= 0:\n",
        "\n",
        "      #=======================================================\n",
        "      # PRE-PROCESSING\n",
        "\n",
        "      # checking number of instruments in a composition\n",
        "      instruments_list = list(set([y[3] for y in events_matrix1]))\n",
        "\n",
        "      if len(events_matrix1) > 0:\n",
        "\n",
        "          #===================================\n",
        "          # ORIGINAL COMPOSITION\n",
        "          #===================================\n",
        "\n",
        "          # Adjusting timings\n",
        "\n",
        "          for e in events_matrix1:\n",
        "            e[1] = int(e[1] / 16)\n",
        "            e[2] = int(e[2] / 16)\n",
        "\n",
        "          # Sorting by patch, pitch, then by start-time\n",
        "\n",
        "          events_matrix1.sort(key=lambda x: x[6])\n",
        "          events_matrix1.sort(key=lambda x: x[4], reverse=True)\n",
        "          events_matrix1.sort(key=lambda x: x[1])\n",
        "\n",
        "          #=======================================================\n",
        "          # FINAL PROCESSING\n",
        "\n",
        "          #=======================================================\n",
        "          # MAIN PROCESSING CYCLE\n",
        "          #=======================================================\n",
        "\n",
        "          pe = events_matrix1[0]\n",
        "\n",
        "          notes = []\n",
        "\n",
        "          for e in events_matrix1:\n",
        "\n",
        "            time = max(0, min(255, (e[1] - pe[1])))\n",
        "            dur = max(0, min(255, e[2]))\n",
        "            cha = max(0, min(15, e[3]))\n",
        "            ptc = max(1, min(127, e[4]))\n",
        "\n",
        "            notes.append([time, dur, cha, ptc])\n",
        "\n",
        "            pe = e\n",
        "\n",
        "mel_pitches = [n[3] for n in notes]\n",
        "mel_tones = [n[3] % 12 for n in notes]"
      ],
      "metadata": {
        "id": "__ucOGq1zHm7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eIIqeaipzVil"
      },
      "outputs": [],
      "source": [
        "# @title Eval the model\n",
        "dtype = 'float16'\n",
        "device_type = 'cuda'\n",
        "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
        "ctx = torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n",
        "\n",
        "model.eval()\n",
        "\n",
        "x = torch.tensor(mel_tones, dtype=torch.long, device='cuda')[None, ...]\n",
        "\n",
        "prime = []\n",
        "\n",
        "for t in mel_tones[:8]:\n",
        "  prime.append(TMIDIX.ALL_CHORDS.index([t]))\n",
        "y = torch.tensor(prime, dtype=torch.long, device='cuda')[None, ...]\n",
        "\n",
        "#x = torch.tensor([[0]] * 1, dtype=torch.long, device='cuda')\n",
        "\n",
        "# run generation\n",
        "\n",
        "with ctx:\n",
        "    out = model.generate(x, y,\n",
        "                        seq_len=x.shape[1]-9,\n",
        "                        temperature=0.85,\n",
        "                        return_prime=True,\n",
        "                        verbose=True)\n",
        "\n",
        "y = out.tolist()\n",
        "\n",
        "print('=' * 70)\n",
        "print(y[0])\n",
        "print('=' * 70)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for c in y[0]:\n",
        "  print(TMIDIX.ALL_CHORDS[c])"
      ],
      "metadata": {
        "id": "l4bjBzU40cQ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mel_tones[:8]"
      ],
      "metadata": {
        "id": "nLyV_Nei4knc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Test model output\n",
        "\n",
        "train_data1 = y[0]\n",
        "\n",
        "#train_data1 = max(melody_chords_f, key = len)\n",
        "\n",
        "print('Sample INTs', train_data1[:15])\n",
        "\n",
        "out = train_data1\n",
        "\n",
        "patches = [0] * 16\n",
        "patches[3] = 40\n",
        "\n",
        "if len(out) != 0:\n",
        "\n",
        "    song = out\n",
        "    song_f = []\n",
        "\n",
        "    time = 0\n",
        "    dur = 0\n",
        "    vel = 90\n",
        "    pitch = 0\n",
        "    channel = 0\n",
        "\n",
        "    mel_idx = 0\n",
        "\n",
        "    for ss in song:\n",
        "\n",
        "        time += notes[mel_idx][0] * 16\n",
        "        dur = notes[mel_idx][1] * 16\n",
        "        pitch = notes[mel_idx][3]\n",
        "        channel = 3\n",
        "        song_f.append(['note', time, dur, channel, pitch, vel ])\n",
        "\n",
        "        chord = TMIDIX.ALL_CHORDS[ss]\n",
        "        for c in chord:\n",
        "          pitch = 48+c\n",
        "          channel = 0\n",
        "          song_f.append(['note', time, dur, channel, pitch, vel ])\n",
        "\n",
        "        mel_idx += 1\n",
        "\n",
        "detailed_stats = TMIDIX.Tegridy_ms_SONG_to_MIDI_Converter(song_f,\n",
        "                                                          output_signature = 'Experimental Music Transformer',\n",
        "                                                          output_file_name = '/content/Experimental-Music-Transformer-Composition',\n",
        "                                                          track_name='Project Los Angeles',\n",
        "                                                          list_of_MIDI_patches=patches\n",
        "                                                          )"
      ],
      "metadata": {
        "id": "o2CUTAAp4bG_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), '/content/model.pth')"
      ],
      "metadata": {
        "id": "OrGEyrNEj_ps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# (TOKENS EMBEDDINGS)"
      ],
      "metadata": {
        "id": "e7xnRcSt5AbX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8745GbH2zYpZ"
      },
      "outputs": [],
      "source": [
        "# @title Explore model tokens embeddings\n",
        "tok_emb = model.decoder.net.token_emb.emb.weight.detach().cpu().tolist()\n",
        "\n",
        "cos_sim = metrics.pairwise_distances(\n",
        "  tok_emb, metric='cosine'\n",
        ")\n",
        "plt.figure(figsize=(7, 7))\n",
        "plt.imshow(cos_sim, cmap=\"inferno\", interpolation=\"nearest\")\n",
        "im_ratio = cos_sim.shape[0] / cos_sim.shape[1]\n",
        "plt.colorbar(fraction=0.046 * im_ratio, pad=0.04)\n",
        "plt.xlabel(\"Position\")\n",
        "plt.ylabel(\"Position\")\n",
        "plt.tight_layout()\n",
        "plt.plot()\n",
        "plt.savefig(\"/content/Experimental-Music-Transformer-Tokens-Embeddings-Plot.png\", bbox_inches=\"tight\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YzCMd94Tu_gz"
      },
      "source": [
        "# Congrats! You did it! :)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}